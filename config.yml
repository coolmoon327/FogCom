# Environment

seed: 88

T: 1000
N_u: 1000
N_m: 50

slot_length: 0.1  # s
block_size: 1 # MB
result_size: 0.001 # MB

csp_num: 5
vm_num: 100 # start from #0
task_freq: 5 # the numebr of tasks per second

follower_strategies_num: 4
# 0 - no bias
# 1 - computation-conservative
# 2 - storage-conservative
# 3 - CSP-preference
alpha: .5
beta: 1000.
M: 1000

tag_len: 4 # the length of test tags
cand_num: 10  # the number of candidates roughly selected by the leader before DRL policy

ganrantee_policy: false # whether use an inner policy to select node for ganrantee when RL fails (selects a Null node)

# Training

batch_size: 512
break_step: 1000  # break training if 'total_step/steps_per_train > break_step'

gamma: 0  # discount factor of future rewards
repeat_times: 4  # repeatedly update network using ReplayBuffer to keep critic's loss small
penalty: -100000. # penalty for wrong selection (Null Node)

force_worker_tag_in_state: -1  # -1: allow env to use the real tag

eval_times: 512

# Miscellaneous

results_path: results
debug_mode: 1
log_pretext: test

# Note: in the fogcom environment, the dict derived from config.yml is also used to sync some global variables:
# n_slot - the current number of slots
# link_check - a LinkCheck object used for retrieve the link state between two nodes
# vm_database - a list storing all the vms' info, index: vmid, value: VM object